//! Unit tests for Qi lexer module
//! æµ‹è¯•è¯æ³•åˆ†æå™¨æ¨¡å—

use qi_compiler::lexer::*;

#[test]
fn test_tokenization_of_basic_tokens() {
    let source = "(){},;:.";
    let mut lexer = Lexer::new(source.to_string());
    let tokens = lexer.tokenize().unwrap();

    let expected_kinds = vec![
        TokenKind::å·¦æ‹¬å·,
        TokenKind::å³æ‹¬å·,
        TokenKind::å·¦å¤§æ‹¬å·,
        TokenKind::å³å¤§æ‹¬å·,
        TokenKind::é€—å·,
        TokenKind::åˆ†å·,
        TokenKind::å†’å·,
        TokenKind::ç‚¹,
        TokenKind::æ–‡ä»¶ç»“æŸ,
    ];

    assert_eq!(tokens.len(), expected_kinds.len());
    for (token, expected) in tokens.iter().zip(expected_kinds.iter()) {
        assert_eq!(token.kind, *expected);
    }
}

#[test]
fn test_chinese_keywords() {
    let source = "å¦‚æœ å¦åˆ™ å½“ å¯¹äº å‡½æ•° è¿”å› å˜é‡ å¸¸é‡ æ•´æ•° å­—ç¬¦ä¸² å¸ƒå°” æµ®ç‚¹æ•°";
    let mut lexer = Lexer::new(source.to_string());
    let tokens = lexer.tokenize().unwrap();

    let expected_keywords = vec![
        TokenKind::å¦‚æœ,
        TokenKind::å¦åˆ™,
        TokenKind::å½“,
        TokenKind::å¯¹äº,
        TokenKind::å‡½æ•°,
        TokenKind::è¿”å›,
        TokenKind::å˜é‡,
        TokenKind::å¸¸é‡,
        TokenKind::æ•´æ•°,
        TokenKind::å­—ç¬¦ä¸²,
        TokenKind::å¸ƒå°”,
        TokenKind::æµ®ç‚¹æ•°,
        TokenKind::æ–‡ä»¶ç»“æŸ,
    ];

    assert_eq!(tokens.len(), expected_keywords.len());
    for (token, expected) in tokens.iter().zip(expected_keywords.iter()) {
        assert_eq!(token.kind, *expected);
    }
}

#[test]
fn test_chinese_identifiers() {
    let source = "å˜é‡å å‡½æ•°å ç”¨æˆ·æ ‡è¯†ç¬¦";
    let mut lexer = Lexer::new(source.to_string());
    let tokens = lexer.tokenize().unwrap();

    // All Chinese characters should be recognized as identifiers
    for token in &tokens[..tokens.len()-1] { // Skip EOF token
        assert_eq!(token.kind, TokenKind::æ ‡è¯†ç¬¦);
    }
}

#[test]
fn test_mixed_chinese_english() {
    let source = "å˜é‡ myVar = 42;";
    let mut lexer = Lexer::new(source.to_string());
    let tokens = lexer.tokenize().unwrap();

    let expected_kinds = vec![
        TokenKind::å˜é‡,
        TokenKind::æ ‡è¯†ç¬¦,
        TokenKind::èµ‹å€¼,
        TokenKind::æ•´æ•°å­—é¢é‡(42),
        TokenKind::åˆ†å·,
        TokenKind::æ–‡ä»¶ç»“æŸ,
    ];

    assert_eq!(tokens.len(), expected_kinds.len());
    for (token, expected) in tokens.iter().zip(expected_kinds.iter()) {
        assert_eq!(token.kind, *expected);
    }
    assert_eq!(tokens[1].text, "myVar");
}

#[test]
fn test_string_literals() {
    let source = r#"å˜é‡ æ¶ˆæ¯ = "ä½ å¥½ï¼Œä¸–ç•Œï¼";"#;
    let mut lexer = Lexer::new(source.to_string());
    let tokens = lexer.tokenize().unwrap();

    assert_eq!(tokens[0].kind, TokenKind::å˜é‡);
    assert_eq!(tokens[1].kind, TokenKind::æ ‡è¯†ç¬¦);
    assert_eq!(tokens[1].text, "æ¶ˆæ¯");
    assert_eq!(tokens[2].kind, TokenKind::èµ‹å€¼);
    assert_eq!(tokens[3].kind, TokenKind::å­—ç¬¦ä¸²å­—é¢é‡);
    assert_eq!(tokens[3].text, "\"ä½ å¥½ï¼Œä¸–ç•Œï¼\"");
}

#[test]
fn test_numeric_literals() {
    let source = "å˜é‡ æ•´æ•° = 42; å˜é‡ æµ®ç‚¹æ•° = 3.14;";
    let mut lexer = Lexer::new(source.to_string());
    let tokens = lexer.tokenize().unwrap();

    assert_eq!(tokens[3].kind, TokenKind::æ•´æ•°å­—é¢é‡(42));
    assert_eq!(tokens[9].kind, TokenKind::æµ®ç‚¹æ•°å­—é¢é‡);
}

#[test]
fn test_character_literals() {
    let source = "'A' 'ä¸­' '\\n'";
    let mut lexer = Lexer::new(source.to_string());
    let tokens = lexer.tokenize().unwrap();

    assert_eq!(tokens.len(), 4); // 3 chars + EOF
    assert!(matches!(tokens[0].kind, TokenKind::å­—ç¬¦å­—é¢é‡('A')));
    assert!(matches!(tokens[1].kind, TokenKind::å­—ç¬¦å­—é¢é‡('ä¸­')));
    assert!(matches!(tokens[2].kind, TokenKind::å­—ç¬¦å­—é¢é‡('\n')));
}

#[test]
fn test_operators() {
    let source = "+ - * / = == != < > <= >=";
    let mut lexer = Lexer::new(source.to_string());
    let tokens = lexer.tokenize().unwrap();

    let expected_operators = vec![
        TokenKind::åŠ ,
        TokenKind::å‡,
        TokenKind::ä¹˜,
        TokenKind::é™¤,
        TokenKind::èµ‹å€¼,
        TokenKind::ç­‰äº,
        TokenKind::ä¸ç­‰äº,
        TokenKind::å°äº,
        TokenKind::å¤§äº,
        TokenKind::å°äºç­‰äº,
        TokenKind::å¤§äºç­‰äº,
        TokenKind::æ–‡ä»¶ç»“æŸ,
    ];

    assert_eq!(tokens.len(), expected_operators.len());
    for (token, expected) in tokens.iter().zip(expected_operators.iter()) {
        assert_eq!(token.kind, *expected);
    }
}

#[test]
fn test_empty_input() {
    let source = "";
    let mut lexer = Lexer::new(source.to_string());
    let tokens = lexer.tokenize().unwrap();

    assert_eq!(tokens.len(), 1);
    assert_eq!(tokens[0].kind, TokenKind::æ–‡ä»¶ç»“æŸ);
}

#[test]
fn test_whitespace_handling() {
    let source = "   \t\n  å˜é‡ x = 1;  \n\t";
    let mut lexer = Lexer::new(source.to_string());
    let tokens = lexer.tokenize().unwrap();

    // Should have: å˜é‡, x, =, 1, ;, æ–‡ä»¶ç»“æŸ
    assert_eq!(tokens.len(), 6);
    assert_eq!(tokens[0].kind, TokenKind::å˜é‡);
    assert_eq!(tokens[1].kind, TokenKind::æ ‡è¯†ç¬¦);
    assert_eq!(tokens[1].text, "x");
}

#[test]
fn test_invalid_character() {
    let source = "å˜é‡ x = @;";
    let mut lexer = Lexer::new(source.to_string());
    let result = lexer.tokenize();

    assert!(result.is_err());
    match result.unwrap_err() {
        LexicalError::InvalidCharacter(char, line, col) => {
            assert_eq!(char, '@');
            assert_eq!(line, 1);
            assert_eq!(col, 11);
        }
        _ => panic!("Expected InvalidCharacter error"),
    }
}

#[test]
fn test_unterminated_string() {
    let source = r#"å˜é‡ æ¶ˆæ¯ = "æœªç»ˆæ­¢å­—ç¬¦ä¸²"#;
    let mut lexer = Lexer::new(source.to_string());
    let result = lexer.tokenize();

    assert!(result.is_err());
    match result.unwrap_err() {
        LexicalError::UnterminatedString(line, col) => {
            assert_eq!(line, 1);
            assert_eq!(col, 7);
        }
        _ => panic!("Expected UnterminatedString error"),
    }
}

#[test]
fn test_comments_are_skipped() {
    let source = r#"
    // This is a line comment
    å˜é‡ x = 5; /* This is a block comment */ å˜é‡ y = 10;
    /// This is a doc comment
    /** This is a doc block comment */
    "#;

    let mut lexer = Lexer::new(source.to_string());
    let tokens = lexer.tokenize().unwrap();

    // Should only tokenize the actual code, not comments
    let expected_kinds = vec![
        TokenKind::å˜é‡,
        TokenKind::æ ‡è¯†ç¬¦,
        TokenKind::èµ‹å€¼,
        TokenKind::æ•´æ•°å­—é¢é‡(5),
        TokenKind::åˆ†å·,
        TokenKind::å˜é‡,
        TokenKind::æ ‡è¯†ç¬¦,
        TokenKind::èµ‹å€¼,
        TokenKind::æ•´æ•°å­—é¢é‡(10),
        TokenKind::åˆ†å·,
        TokenKind::æ–‡ä»¶ç»“æŸ,
    ];

    assert_eq!(tokens.len(), expected_kinds.len());
    for (token, expected) in tokens.iter().zip(expected_kinds.iter()) {
        assert_eq!(token.kind, *expected);
    }
}

#[test]
fn test_span_information() {
    let source = "å˜é‡ x = 42;";
    let mut lexer = Lexer::new(source.to_string());
    let tokens = lexer.tokenize().unwrap();

    // Test that span information is correctly recorded
    assert_eq!(tokens[0].span.start, 0); // "å˜"
    assert_eq!(tokens[0].span.end, 2);   // after "é‡"
    assert_eq!(tokens[0].line, 1);
    assert_eq!(tokens[0].column, 1);

    assert_eq!(tokens[1].span.start, 3); // "x"
    assert_eq!(tokens[1].span.end, 4);
    assert_eq!(tokens[1].line, 1);
    assert_eq!(tokens[1].column, 4);
}

#[test]
fn test_line_and_column_tracking() {
    let source = "å˜é‡ x = 1;\nå˜é‡ y = 2;";
    let mut lexer = Lexer::new(source.to_string());
    let tokens = lexer.tokenize().unwrap();

    // First line
    assert_eq!(tokens[0].line, 1);
    assert_eq!(tokens[0].column, 1);

    // Second line (after newline)
    let newline_token = tokens.iter().find(|t| t.line == 2).unwrap();
    assert_eq!(newline_token.line, 2);
    assert_eq!(newline_token.column, 1);
}

#[test]
fn test_diagnostics_collection() {
    let source = "å˜é‡ x = 5 @ 3;";
    let mut lexer = Lexer::new(source.to_string());
    let result = lexer.tokenize();

    assert!(result.is_err());

    // Check that diagnostics were collected
    let diagnostics = lexer.diagnostics();
    assert!(diagnostics.error_count() > 0);

    let formatted = lexer.format_diagnostics();
    assert!(formatted.contains("æ— æ•ˆå­—ç¬¦"));
}

#[test]
fn test_error_summary() {
    let source = "å˜é‡ x = 5 @ 3;";
    let mut lexer = Lexer::new(source.to_string());
    let _result = lexer.tokenize().unwrap_err();

    let (error_count, warning_count) = lexer.get_error_summary();
    assert!(error_count > 0);
    assert_eq!(warning_count, 0);

    assert!(lexer.has_critical_errors());
}

#[test]
fn test_complex_source_tokenization() {
    let source = r#"
    å‡½æ•° è®¡ç®—æ€»å’Œ(æ•°ç»„) {
        å˜é‡ æ€»å’Œ = 0;
        å¯¹äº å…ƒç´  åœ¨ æ•°ç»„ {
            æ€»å’Œ = æ€»å’Œ + å…ƒç´ ;
        }
        è¿”å› æ€»å’Œ;
    }

    å¦‚æœ æ€»å’Œ > 100 {
        è¿”å› "å¤§äº100";
    } å¦åˆ™ {
        è¿”å› "å°äºç­‰äº100";
    }
    "#;

    let mut lexer = Lexer::new(source.to_string());
    let tokens = lexer.tokenize().unwrap();

    // Should tokenize successfully
    assert!(!tokens.is_empty());
    assert_eq!(tokens.last().unwrap().kind, TokenKind::æ–‡ä»¶ç»“æŸ);

    // Should contain expected keywords
    let token_kinds: Vec<_> = tokens.iter().map(|t| &t.kind).collect();
    assert!(token_kinds.contains(&&TokenKind::å‡½æ•°));
    assert!(token_kinds.contains(&&TokenKind::å¯¹äº));
    assert!(token_kinds.contains(&&TokenKind::è¿”å›));
    assert!(token_kinds.contains(&&TokenKind::å¦‚æœ));
    assert!(token_kinds.contains(&&TokenKind::å¦åˆ™));
}

#[test]
fn test_arrows_and_special_tokens() {
    let source = "->";
    let mut lexer = Lexer::new(source.to_string());
    let tokens = lexer.tokenize().unwrap();

    assert_eq!(tokens.len(), 2); // -> + EOF
    assert_eq!(tokens[0].kind, TokenKind::ç®­å¤´);
}

#[test]
fn test_unicode_support() {
    let source = "å˜é‡ ä¸­æ–‡å˜é‡å = 'ä¸­';\nå­—ç¬¦ä¸² ğŸš€ emoji = \"ç«ç®­\";";
    let mut lexer = Lexer::new(source.to_string());
    let tokens = lexer.tokenize().unwrap();

    // Should handle Unicode characters properly
    assert!(tokens.iter().any(|t| t.text.contains("ä¸­æ–‡å˜é‡å")));
    assert!(tokens.iter().any(|t| t.text.contains("ğŸš€")));
    assert!(tokens.iter().any(|t| t.text.contains("ç«ç®­")));
}